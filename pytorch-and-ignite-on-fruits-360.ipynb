{"cells":[{"metadata":{"_uuid":"e6e64edb37cf58e6caca8b7d98252e72fb6978f5"},"cell_type":"markdown","source":"# Fruits 360 dataset with PyTorch and Ignite\n\nIn this kernel I would like to present recently released the first version of high-level library [*ignite*](https://github.com/pytorch/ignite) to help training neural networks in PyTorch.\n\n\n## Why to use *ignite* ?\n\n- ignite helps you write compact but full-featured training loops in a few lines of code\n- you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n\n## Installation\n\nJust run the following command:\n```bash\npip install pytorch-ignite\n```\nor with conda\n```bash\nconda install ignite -c pytorch\n```\n\nThe latest version can be installed from the [github](https://github.com/pytorch/ignite.git):\n```bash\ngit clone https://github.com/pytorch/ignite.git\ncd ignite && python setup.py install\n```"},{"metadata":{"trusted":true,"_uuid":"ef8f69fc3ba5953edbddcf5822045d87cee06b1f"},"cell_type":"code","source":"# Let's install ignite as a custom package:\n!pip install git+https://github.com/pytorch/ignite.git --prefix=/kaggle/working\n    \nimport sys\nsys.path.insert(0, \"/kaggle/working/lib/python3.6/site-packages\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85f35e858fc5884886611c5ecce8e2f3bab8eaac"},"cell_type":"markdown","source":"Before we starts with *ignite*, let's define essential things: \n- dataflow :\n    - train data loader\n    - validation data loader\n- model :\n   - let's take a small network SqueezeNet \n- optimizer : \n   - let's take SGD\n- loss function :\n    - Cross-Entropy"},{"metadata":{"_kg_hide-input":true,"_uuid":"085338e05b35364bdfad806f8554810366b9588b","trusted":true},"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import Subset\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Compose, RandomResizedCrop, RandomVerticalFlip, RandomHorizontalFlip\nfrom torchvision.transforms import ColorJitter, ToTensor, Normalize\n\n\nFRUIT360_PATH = Path(\".\").resolve().parent / \"input\" / \"fruits-360_dataset\" / \"fruits-360\"\n\nimg_size = 64\n\ndevice = \"cuda\"\nif not torch.cuda.is_available():\n    device = \"cpu\"\n\ntrain_transform = Compose([\n    RandomHorizontalFlip(),    \n    RandomResizedCrop(size=img_size),\n    ColorJitter(brightness=0.12),\n    ToTensor(),\n    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nval_transform = Compose([\n    RandomResizedCrop(size=img_size),\n    ToTensor(),\n    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nbatch_size = 128\nnum_workers = 8\n\ntrain_dataset = ImageFolder((FRUIT360_PATH /\"Training\").as_posix(), transform=train_transform, target_transform=None)\nval_dataset = ImageFolder((FRUIT360_PATH /\"Test\").as_posix(), transform=val_transform, target_transform=None)\n\npin_memory = \"cuda\" in device\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n                          drop_last=True, pin_memory=pin_memory)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n                        drop_last=False, pin_memory=pin_memory)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c63174435b431d5cda17a8ea49f0f0f5f653e2e0","trusted":true},"cell_type":"code","source":"print(\"PyTorch version: {} | Device: {}\".format(torch.__version__, device))\nprint(\"Train loader: num_batches={} | num_samples={}\".format(len(train_loader), len(train_loader.sampler)))\nprint(\"Validation loader: num_batches={} | num_samples={}\".format(len(val_loader), len(val_loader.sampler)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"59cad824744cdc334b0074535d053cda6a0bdd9e","trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision.models.squeezenet import squeezenet1_1\nfrom torch.optim import SGD","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96b06dffc097196718ec7ea799146b2fed6ef829","trusted":true},"cell_type":"code","source":"model = squeezenet1_1(pretrained=False, num_classes=81)\nmodel.classifier[-1] = nn.AdaptiveAvgPool2d(1)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0e3456ae912799f526d2076a48076b3b8b871a2","trusted":true},"cell_type":"code","source":"optimizer = SGD(model.parameters(), lr=0.01, momentum=0.5)\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"887968154e1949db79cadc6f2b7de37518e33c4d"},"cell_type":"markdown","source":"And let us begin\n\n## Ignite quickstart with Fruits 360 dataset\n\n### Engine\n\nThe base of the framework is `ignite.engine.Engine`, an object that loops a given number of times over provided data, executes a processing function and returns a result:\n```python\nwhile epoch < max_epochs:\n    # run once on data\n    for batch in data:\n        output = process_function(batch)\n```\n\nSo, a model trainer is simply an engine that loops multiple times over the training dataset and updates model parameters. \nSimilarly, model evaluation can be done with an engine that runs a single time over the validation dataset and computes metrics."},{"metadata":{"_uuid":"81769c0904de73b324ffa616fd6e66bdfbaf78be","trusted":true},"cell_type":"code","source":"from ignite.engine import Engine, _prepare_batch, create_supervised_trainer\n\ndef model_update(engine, batch):\n    model.train()\n    optimizer.zero_grad()\n    x, y = _prepare_batch(batch, device=device)\n    y_pred = model(x)\n    loss = criterion(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ntrainer = Engine(model_update)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26c84d93884195bde229671f9187e39977b059bb"},"cell_type":"markdown","source":"and that's it. A trainer is setup, so we can just simply execute `run` method and our model will be silently trained. We could also use a helper method `ignite.engine.create_supervised_trainer` to create a trainer without explicitly coding `model_update` function:\n```python\nfrom ignite.engine import create_supervised_trainer\n\ntrainer = create_supervised_trainer(model, optimizer, criterion, device)\n```\n\n\n> **Note:** update function should have two inputs : `engine` and `batch`\n\n\n\nLet's add more interaction with our created trainer:\n- add logging of loss function value every 50 iterations\n- run offline metrics computation on a subset of the training dataset\n- run metrics computation on the validation dataset once epoch is finished\n- checkpoint trained model every epoch\n- save 3 best models\n- add LR scheduling\n- add early stopping"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"### Events and Handlers\n\nIn order to accomplish above todo list *ignite* provides an event system that facilitates interaction at each step of the run:\n- *engine is started/completed*\n- *epoch is started/completed*\n- *batch iteration is started/completed*\n\nSo that user can execute a custom code as an event handler.\n\n#### Training batch loss logging\n\nWe just define a function and add this function as a handler to the trainer. There are two ways to add a handler: via `add_event_handler`, via `on` decorator:"},{"metadata":{"_uuid":"1b38f21bcca324188747acc2cfba5d8ad6b42c9b","trusted":true},"cell_type":"code","source":"from ignite.engine import Events\n\nlog_interval = 50 \nif 'cpu' in device:\n    log_interval = 5 \n\n@trainer.on(Events.ITERATION_COMPLETED)\ndef log_training_loss(engine):\n    iteration = (engine.state.iteration - 1) % len(train_loader) + 1\n    if iteration % log_interval == 0:\n        print(\"Epoch[{}] Iteration[{}/{}] Loss: {:.4f}\".format(engine.state.epoch, iteration, len(train_loader), engine.state.output))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0408630fbed51cb94f74100b170c51d3dc1b0dc2"},"cell_type":"markdown","source":"The same can be done with `add_event_handler` like this:\n```python\ntrainer.add_event_handler(Events.ITERATION_COMPLETED, log_training_loss)\n```\n\n\n> **Note:** handlers can also pass `args` and `kwargs`, so in general a handler can be defined as \n\n```python\n    def custom_handler(engine, *args, **kwargs):\n        pass\n\n    trainer.add_event_handler(Events.ITERATION_COMPLETED, custom_handler, *args, **kwargs)\n    # or \n    @trainer.on(Events.ITERATION_COMPLETED, *args, **kwargs)\n    def custom_handler(engine, *args, **kwargs):\n        pass\n```\n\nLet's see what happens if we run the trainer for a single epoch"},{"metadata":{"_uuid":"e8e38ea4e67cf0bd1dcca5420eef885358940e9e","trusted":true},"cell_type":"code","source":"output = trainer.run(train_loader, max_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87f527b6b726e138b49267a6aeb3ffc8a9f679c2"},"cell_type":"markdown","source":"Looks good! \n\n> add logging of loss function value every 50 iterations\n\nDone!\n\n#### Offline training metrics and validation metrics\n\nNow let's add some code to compute metrics: average accuracy, precision, recall over a subset of the training dataset and validation dataset. What is *offline* training metrics and why ? By offline, I mean that we compute training metrics using a fixed model vs online when metrics are computed batchwise over model that keep changing every iteration.\n\nAt first we define metrics we want to compute:"},{"metadata":{"_uuid":"5ffbc59fca196242f32e38827b7f548a51a4754e","trusted":true},"cell_type":"code","source":"from ignite.metrics import Loss, CategoricalAccuracy, Precision, Recall\n\n\nmetrics = {\n    'avg_loss': Loss(criterion),\n    'avg_accuracy': CategoricalAccuracy(),\n    'avg_precision': Precision(average=True), \n    'avg_recall': Recall(average=True)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a903a90ce75b7d537d45cf789569e1b1b278c8b1"},"cell_type":"markdown","source":"Next we can define engines using a helper method `ignite.engine.create_supervised_evaluator`:"},{"metadata":{"_uuid":"161d95906d2bb002365409564e7900f054dee28f","trusted":true},"cell_type":"code","source":"from ignite.engine import create_supervised_evaluator\n\ntrain_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\nval_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96d190708d82bb588dcde135cd7736a47fdd76aa"},"cell_type":"markdown","source":"and we need to define a train subset and its data loader:"},{"metadata":{"_uuid":"5a2d3f6d2687923585099ea643483c6be57d24bf","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data.dataset import Subset\n\nrandom_indices = np.random.permutation(np.arange(len(train_dataset)))[:len(val_dataset)]\ntrain_subset = Subset(train_dataset, indices=random_indices)\n\ntrain_eval_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n                               drop_last=True, pin_memory=\"cuda\" in device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e59eec66fc083cbbf3ee601f7b16a3d90630d4e"},"cell_type":"markdown","source":"Now let's define when to execute metrics computation and display results"},{"metadata":{"_uuid":"ea523100429cf28ed88d29c323ca1c6255b454f4","trusted":true},"cell_type":"code","source":"@trainer.on(Events.EPOCH_COMPLETED)\ndef compute_and_display_offline_train_metrics(engine):\n    epoch = engine.state.epoch\n    print(\"Compute train metrics...\")\n    metrics = train_evaluator.run(train_eval_loader).metrics\n    print(\"Training Results - Epoch: {}  Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}\"\n          .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall']))\n    \n    \n@trainer.on(Events.EPOCH_COMPLETED)\ndef compute_and_display_val_metrics(engine):\n    epoch = engine.state.epoch\n    print(\"Compute validation metrics...\")\n    metrics = val_evaluator.run(val_loader).metrics\n    print(\"Validation Results - Epoch: {}  Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}\"\n          .format(engine.state.epoch, metrics['avg_loss'], metrics['avg_accuracy'], metrics['avg_precision'], metrics['avg_recall']))    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dcd01508d6925b11ab77d520955a7c94ef59dc4"},"cell_type":"markdown","source":"Let's check it again"},{"metadata":{"_uuid":"0acaa445bcabc608b8139a1b8a6b132608b69026","trusted":true},"cell_type":"code","source":"output = trainer.run(train_loader, max_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6de81a2fd6a267bdb2e0bbdd44ae94a1dcb92742"},"cell_type":"markdown","source":"Nice !\n\n> run offline metrics computation on a subset of the training dataset\n\n> run metrics computation on the validation dataset once epoch is finished\n\nDone !"},{"metadata":{"_uuid":"624c285ac6e756f3e264cbed2deb4e9b912977e3"},"cell_type":"markdown","source":"----\n\n##### More details\n\n\n\nLet's explain some details in the above code. Maybe you've remarked the following\n```python\nmetrics = train_evaluator.run(train_eval_loader).metrics\n```\nand you have a question what is the object returned by `train_evaluator.run(train_eval_loader)` that has `metrics` as attribute. \n\nActually, `Engine` contains a structure called `State` to pass data between handlers. Basically, `State` contains information on the current \nepoch, iteration, max epochs, etc and also can be used to pass some custom data, such as metrics. Thus, the above code can be rewritten as \n```python\nstate = train_evaluator.run(train_eval_loader)\nmetrics = state.metrics\n# or just\ntrain_evaluator.run(train_eval_loader)\nmetrics = train_evaluator.state.metrics\n```\n\n-----"},{"metadata":{"_uuid":"4eda5ec09e175404ee59a89706635413ed923570"},"cell_type":"markdown","source":"#### Learning rate scheduling\n\nThere are several ways to perform learning rate scheduling with *ignite*, here we will use the most simple one by calling `lr_scheduler.step()` every epoch:"},{"metadata":{"_uuid":"7217a70aa70774308278d651de4c95f38dbb120e","trusted":true},"cell_type":"code","source":"from torch.optim.lr_scheduler import ExponentialLR\n\n\nlr_scheduler = ExponentialLR(optimizer, gamma=0.8)\n\n\n@trainer.on(Events.EPOCH_STARTED)\ndef update_lr_scheduler(engine):\n    lr_scheduler.step()\n    # Display learning rate:\n    if len(optimizer.param_groups) == 1:\n        lr = float(optimizer.param_groups[0]['lr'])\n        print(\"Learning rate: {}\".format(lr))\n    else:\n        for i, param_group in enumerate(optimizer.param_groups):\n            lr = float(param_group['lr'])\n            print(\"Learning rate (group {}): {}\".format(i, lr))    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57b2433b79e1baa345376828bde594e57f736032"},"cell_type":"markdown","source":"#### Training checkpointing\n\nAs we move on training, we would like to store the best model, last trained model, optimizer and learning rate scheduler. With *ignite* it is not a problem, there is a special class `ModelCheckpoint` for these purposes. \n\nLet's use `ModelCheckpoint` handler to store the best model defined by validation accuracy. In this case we define a `score_function` that provides validation accuracy to the handler and it decides (max value - better) whether to save or not the model."},{"metadata":{"_uuid":"2a0ee3f0b7d7387d9914c274607760fc28b19655","trusted":true},"cell_type":"code","source":"from ignite.handlers import ModelCheckpoint\n\n\ndef score_function(engine):\n    val_avg_accuracy = engine.state.metrics['avg_accuracy']\n    # Objects with highest scores will be retained.\n    return val_avg_accuracy\n\n\nbest_model_saver = ModelCheckpoint(\"best_models\",  # folder where to save the best model(s)\n                                   filename_prefix=\"model\",  # filename prefix -> {filename_prefix}_{name}_{step_number}_{score_name}={abs(score_function_result)}.pth\n                                   score_name=\"val_accuracy\",  \n                                   score_function=score_function,\n                                   n_saved=3,\n                                   atomic=True,  # objects are saved to a temporary file and then moved to final destination, so that files are guaranteed to not be damaged\n                                   save_as_state_dict=True,  # Save object as state_dict\n                                   create_dir=True)\n\nval_evaluator.add_event_handler(Events.COMPLETED, best_model_saver, {\"best_model\": model})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d61be4bfcbba65becf9a56033cdc3e55108d963c"},"cell_type":"markdown","source":"Now let's define another `ModelCheckpoint` handler to store trained model, optimizer and lr scheduler every 1000 iterations:"},{"metadata":{"_uuid":"790337775b67543f3df264c104ce8a9a2483cdbf","trusted":true},"cell_type":"code","source":"training_saver = ModelCheckpoint(\"checkpoint\",\n                                 filename_prefix=\"checkpoint\",\n                                 save_interval=1000,\n                                 n_saved=1,\n                                 atomic=True,\n                                 save_as_state_dict=True,\n                                 create_dir=True)\n\nto_save = {\"model\": model, \"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler} \ntrainer.add_event_handler(Events.ITERATION_COMPLETED, training_saver, to_save)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbdcb311713b3e3ad05f936d7a9d9525055b6ae6"},"cell_type":"markdown","source":"We are almost done with preparations and a cherry on top\n\n#### Early stopping\n\nLet's add another handler to stop training if model fails to improve a score defined by a `score_function` during 10 epochs:"},{"metadata":{"_uuid":"60c08fbae224364b4b089d8ab34e4860d57c3d7e","trusted":true},"cell_type":"code","source":"from ignite.handlers import EarlyStopping\n\nearly_stopping = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)\n\nval_evaluator.add_event_handler(Events.EPOCH_COMPLETED, early_stopping)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3acbcc0f3ed150a978067dfa23358646fa5bb0"},"cell_type":"markdown","source":"## Run training\n\nNow we can just call `run` method and train model during a number of epochs "},{"metadata":{"_uuid":"ecd0e705a55519e0e4fe700a4e729bdb5ec5da86","trusted":true},"cell_type":"code","source":"max_epochs = 10\n\noutput = trainer.run(train_loader, max_epochs=max_epochs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cba32686a19d34d49cb8be58c6b0c35abc54d445"},"cell_type":"markdown","source":"Let's check saved 3 best models and the checkpoint:"},{"metadata":{"trusted":true,"_uuid":"bf556501d1367c628b97c2afc95593834994003a"},"cell_type":"code","source":"!ls best_models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61671473174dba42ba6ebeba8ccf6bcc1ed0e1e1"},"cell_type":"code","source":"!ls checkpoint/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0273327c86e60a3f6b52958c8423aff0e535899f"},"cell_type":"markdown","source":"## Inference\n\nLet's first create a test dataloader from validation dataset such that provided batch is composed of `(samples, sample_indices)`:"},{"metadata":{"trusted":true,"_uuid":"5b933e90e724000e36a0b5007f6efbb532154e3c"},"cell_type":"code","source":"class TestDataset(Dataset):\n    \n    def __init__(self, ds):\n        self.ds = ds\n        \n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, index):\n        return self.ds[index][0], index\n\n    \ntest_dataset = TestDataset(val_dataset)\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, \n                         drop_last=False, pin_memory=\"cuda\" in device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdfe1e2216beac80799d2a65d225ab4cf016e70d"},"cell_type":"markdown","source":"With ignite to implement an engine that inference on data is simple. Similarly when we created an evaluation engine, now we modify the update function to store output results. We will also perform what is called test time augmentation (TTA)."},{"metadata":{"trusted":true,"_uuid":"4e5d15fd3f43b0dc89ff9724a7061534ae285b01"},"cell_type":"code","source":"import torch.nn.functional as F\nfrom ignite._utils import convert_tensor\n\n\ndef _prepare_batch(batch):\n    x, index = batch\n    x = convert_tensor(x, device=device)\n    return x, index\n\n\ndef inference_update(engine, batch):\n    x, indices = _prepare_batch(batch)\n    y_pred = model(x)\n    y_pred = F.softmax(y_pred, dim=1)\n    return {\"y_pred\": convert_tensor(y_pred, device='cpu'), \"indices\": indices}\n\n    \nmodel.eval()\ninferencer = Engine(inference_update)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b7e4aa06b33d1e3aca0b762674b248359632ae8"},"cell_type":"markdown","source":"Next let's define a handler to log steps during the inference and a handler to store predictions"},{"metadata":{"trusted":true,"_uuid":"a12c7a5571a645e4b63602edbe15bb355dcbc0ab"},"cell_type":"code","source":"@inferencer.on(Events.EPOCH_COMPLETED)\ndef log_tta(engine):\n    print(\"TTA {} / {}\".format(engine.state.epoch, n_tta))\n\n    \nn_tta = 3\nnum_classes = 81\nn_samples = len(val_dataset)\n\n# Array to store prediction probabilities\ny_probas_tta = np.zeros((n_samples, num_classes, n_tta), dtype=np.float32)\n\n# Array to store sample indices\nindices = np.zeros((n_samples, ), dtype=np.int)\n    \n\n@inferencer.on(Events.ITERATION_COMPLETED)\ndef save_results(engine):\n    output = engine.state.output\n    tta_index = engine.state.epoch - 1\n    start_index = ((engine.state.iteration - 1) % len(test_loader)) * batch_size\n    end_index = min(start_index + batch_size, n_samples)\n    batch_y_probas = output['y_pred'].detach().numpy()\n    y_probas_tta[start_index:end_index, :, tta_index] = batch_y_probas\n    if tta_index == 0:\n        indices[start_index:end_index] = output['indices']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8e7d13bb51f0d0abbe585c95128f37a0acd34ee"},"cell_type":"markdown","source":"Before running the inference, we may want to load the best model from the storage:\n```python\nmodel = squeezenet1_1(pretrained=False, num_classes=64)\nmodel.classifier[-1] = nn.AdaptiveAvgPool2d(1)  # Adapt the last average pooling to our data\nmodel = model.to(device)\n\nmodel_state_dict = torch.load(\"best_models/model_best_model_N_val_accuracy=0.XYZ.pth\")\nmodel.load_state_dict(model_state_dict)\n```"},{"metadata":{"trusted":true,"_uuid":"cb14b79cf32f32137355e91fbd742cdb07d1255d"},"cell_type":"code","source":"inferencer.run(test_loader, max_epochs=n_tta)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"526507b808de5744e9b47029d85e3fd63b6d0ea1"},"cell_type":"markdown","source":"Final probability aggregation can be done using mean or gmean"},{"metadata":{"trusted":true,"_uuid":"e76f45829558cc6bb81d410a8363be78b881fc66"},"cell_type":"code","source":"y_probas = np.mean(y_probas_tta, axis=-1)\ny_preds = np.argmax(y_probas, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01ffb1a76179678876669f0a55c315754e8c74af"},"cell_type":"markdown","source":"Next step can be to create a submission using `indices` and `y_probas`. Here we will just compute accuracy on our test=validation dataset"},{"metadata":{"trusted":true,"_uuid":"b6b5c6406dbb87829290ffb20377c3951a83cb2d"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_test_true = [y for _, y in val_dataset]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27f2a13c7601ad254702579ebbee126620ea8074"},"cell_type":"code","source":"accuracy_score(y_test_true, y_preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ee5b8380dd53ce5afef93cf6df6c80ead3ced1e"},"cell_type":"markdown","source":"### Final words\n\nThat's all for this kernel. If you liked it - please upvote. \n\nIf you liked *ignite*, please visit its [documentation site](https://pytorch.org/ignite/), [github code](https://github.com/pytorch/ignite) and checkout [examples](https://github.com/pytorch/ignite/tree/master/examples) with `tensorboard`, `visdom` integration and how to train dcgan. Some other examples can be found [here](https://github.com/vfdev-5/ignite-examples). \n\nWe are actively working on it and appreciate all contributions and feedbacks. As always, PR are very welcome! \n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"33db439f725e139494d97c2e830daf3aa7bcecc2"},"cell_type":"code","source":"# Remove output to be able to commit\n!rm -R best_models/ checkpoint/ lib/","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}